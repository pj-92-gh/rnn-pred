{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_setups():\n",
    "    text_file = 'reverse_dois_out_final.txt'\n",
    "    seq_length = 64\n",
    "\n",
    "    text = open(text_file, 'r').read()\n",
    "    vocab = sorted(set(text))\n",
    "    char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "    idx2char = np.array(vocab)\n",
    "    text_as_int = np.array([char2idx[c] for c in text])\n",
    "    examples_per_epoch = len(text)//(seq_length+1)\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "    sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    \n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = .3\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Recreate the exact same model, including its weights and the optimizer\n",
    "    new_model = tf.keras.models.load_model('test_model_save.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elie Nasr: Hello from the ground before the playoffs and started to the problem with a pretty good man\n",
      "Elie Nasr: It's a good day of a surface of a bit boring\n",
      "Elie Nasr: It was a good morning and then the preserves of the start of the ground and some trash can do that\n",
      "Elie Nasr: It's not a good day\n",
      "Elie Nasr: I don't want to be a good time to get the back to back to the player\n",
      "Elie Nasr: The bald men are so good\n",
      "Pranav Jayanth: the bald men and some old ass shot\n",
      "Elie Nasr: I don't know what they stop\n",
      "Elie Nasr: It's a good shot\n",
      "Elie Nasr: I don't know what the fuck is this\n",
      "Pranav Jayanth: i don’t think the bald men and then the player of the announcer and then the plane is a long time before the time or something\n",
      "Pranav Jayanth: it was a good day of the bet\n",
      "Pranav Jayanth: i’m gonna be a back to the game\n",
      "Elie Nasr: I don't know what I'm doing\n",
      "Elie Nasr: It is a good song\n",
      "Elie Nasr: I don't even need to get to the playoffs and then the bald men and the patriots win the problem\n",
      "Elie Nasr: It's gone for \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(new_model, start_string=u\"Elie Nasr: Hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
