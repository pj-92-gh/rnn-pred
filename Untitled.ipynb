{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "def generate_text(model, start_string):\n",
    "    text_file = 'reverse_dois_out_final.txt'\n",
    "    seq_length = 64\n",
    "    text = open(text_file, 'r').read()\n",
    "    vocab = sorted(set(text))\n",
    "    char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "    idx2char = np.array(vocab)\n",
    "    text_as_int = np.array([char2idx[c] for c in text])\n",
    "    examples_per_epoch = len(text)//(seq_length+1)\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "    sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "    \n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  \n",
    "    num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "    temperature = .3\n",
    "\n",
    "  # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "def use_model(userinput):\n",
    "    new_model = tf.keras.models.load_model('test_model_save.h5', compile=False)\n",
    "    s = (generate_text(new_model, start_string=userinput))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing and the bass been trying to get to the game of the player of the contract\n",
      "Elie Nasr: I have a long time\n",
      "Elie Nasr: I don't even know what the fuck is that\n",
      "Pranav Jayanth: i think the problem is a problem in the middle of shit\n",
      "Pranav Jayanth: i was a good game to make it to the trail of a picture of a pussy beans\n",
      "Pranav Jayanth: i’m sure you are not gonna be a good day\n",
      "Pranav Jayanth: i don’t know what the hell dude\n",
      "Pranav Jayanth: i don’t know what the fuck do you have to get a new destroy the hole then the present a link.https://www.youtube.com/watch?v=0Xjlket-guard-is-trash-can-you-eat-a-thing-to-the-more-thing------ my contract of the top of the thing\n",
      "Pranav Jayanth: i started a conservative than the destroy of the playoffs and the bald men and the other stand some sort of persona line at the end\n",
      "Elie Nasr: I think it was a good game of the bet\n",
      "Elie Nasr: It's not a bad show\n",
      "Elie Nasr: That's what I meant to the present\n",
      "Aditya Batchu: I think the bald men and then the cavs are p\n"
     ]
    }
   ],
   "source": [
    "s = use_model('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "myjson = json.dumps(use_model('Pranav Jayanth: This is a test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Pranav Jayanth: This is a test thing to do that\\\\nElie Nasr: I can\\'t believe the best thing is cool\\\\nElie Nasr: It\\'s too bad that is wrong\\\\nElie Nasr: I don\\'t want to be a bald man\\\\nElie Nasr: It was a good day\\\\nElie Nasr: I don\\'t think that was a bad time\\\\nElie Nasr: It\\'s a good game\\\\nElie Nasr: I don\\'t know what I was trying to do that\\\\nElie Nasr: I like the bourbon\\\\nElie Nasr: I don\\'t know what the fuck\\\\nElie Nasr: I don\\'t know what the fuck is this greek\\\\nPranav Jayanth: i don\\\\u2019t even know what it is\\\\nPranav Jayanth: i was trying to be a back to the game of the police\\\\nElie Nasr: I don\\'t know what the fuck is this\\\\nElie Nasr: I can\\'t believe the doctor is a good time to be able to stop an accounting for me\\\\nElie Nasr: I don\\'t know what the hell dude\\\\nElie Nasr: I don\\'t even know what the fuck is that\\\\nElie Nasr: I don\\'t know what they don\\'t get the ball and then the present to the game of the playoffs and the raps are probably a man\\\\nElie Nasr: I don\\'t want to stay up to be a good track\\\\nElie Nasr: It was a good time\\\\nElie Nasr: I d\"'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
